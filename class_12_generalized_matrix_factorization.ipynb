{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display, HTML\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Fix the dying kernel problem (only a problem in some installations - you can remove it, if it works without it)\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-tourist",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_ratings_df = pd.read_csv(os.path.join(\"data\", \"movielens_small\", \"ratings.csv\")).rename(columns={'userId': 'user_id', 'movieId': 'item_id'})\n",
    "ml_movies_df = pd.read_csv(os.path.join(\"data\", \"movielens_small\", \"movies.csv\")).rename(columns={'movieId': 'item_id'})\n",
    "ml_df = pd.merge(ml_ratings_df, ml_movies_df, on='item_id')\n",
    "\n",
    "# Filter the data to reduce the number of movies\n",
    "seed = 6789\n",
    "rng = np.random.RandomState(seed=seed)\n",
    "left_ids = rng.choice(ml_movies_df['item_id'], size=90, replace=False)\n",
    "left_ids = list(set(left_ids).union(set([1, 318, 1193, 1208, 1214, 1721, 2959, 3578, 4306, 109487])))\n",
    "\n",
    "ml_ratings_df = ml_ratings_df.loc[ml_ratings_df['item_id'].isin(left_ids)]\n",
    "ml_movies_df = ml_movies_df.loc[ml_movies_df['item_id'].isin(left_ids)]\n",
    "ml_df = ml_df.loc[ml_df['item_id'].isin(left_ids)]\n",
    "\n",
    "display(HTML(ml_movies_df.head(10).to_html()))\n",
    "\n",
    "print(\"Number of interactions left: {}\".format(len(ml_ratings_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-brooklyn",
   "metadata": {},
   "source": [
    "## Shift item ids and user ids so that they are consecutive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = ml_ratings_df.copy()\n",
    "\n",
    "unique_item_ids = interactions_df['item_id'].unique()\n",
    "item_id_mapping = dict(zip(unique_item_ids, list(range(len(unique_item_ids)))))\n",
    "item_id_reverse_mapping = dict(zip(list(range(len(unique_item_ids))), unique_item_ids))\n",
    "unique_user_ids = interactions_df['user_id'].unique()\n",
    "user_id_mapping = dict(zip(unique_user_ids, list(range(len(unique_user_ids)))))\n",
    "user_id_reverse_mapping = dict(zip(list(range(len(unique_user_ids))), unique_user_ids))\n",
    "\n",
    "interactions_df['item_id'] = interactions_df['item_id'].map(item_id_mapping)\n",
    "interactions_df['user_id'] = interactions_df['user_id'].map(user_id_mapping)\n",
    "\n",
    "display(interactions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-meeting",
   "metadata": {},
   "source": [
    "## Get the number of items and users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = np.max(interactions_df['item_id']) + 1\n",
    "n_users = np.max(interactions_df['user_id']) + 1\n",
    "\n",
    "print(\"n_items={}\\nn_users={}\".format(n_items, n_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-threshold",
   "metadata": {},
   "source": [
    "## Get the user-item interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping to int is necessary because of how iterrows works\n",
    "r = np.zeros(shape=(n_users, n_items))\n",
    "for idx, interaction in interactions_df.iterrows():\n",
    "    r[int(interaction['user_id'])][int(interaction['item_id'])] = 1\n",
    "    \n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-little",
   "metadata": {},
   "source": [
    "## Generate negative interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neg_per_pos = 5\n",
    "interactions_pos_neg_df = interactions_df[['user_id', 'item_id']].copy()\n",
    "\n",
    "# Indicate positive interactions\n",
    "\n",
    "interactions_pos_neg_df['interacted'] = 1\n",
    "\n",
    "# Generate negative interactions\n",
    "\n",
    "negative_interactions = []\n",
    "\n",
    "i = 0\n",
    "while i < n_neg_per_pos * len(interactions_df):\n",
    "    sample_size = 1000\n",
    "    user_ids = rng.choice(np.arange(n_users), size=sample_size)\n",
    "    item_ids = rng.choice(np.arange(n_items), size=sample_size)\n",
    "\n",
    "    j = 0\n",
    "    while j < sample_size and i < n_neg_per_pos * len(interactions_df):\n",
    "        if r[user_ids[j]][item_ids[j]] == 0:\n",
    "            negative_interactions.append([user_ids[j], item_ids[j], 0])\n",
    "            i += 1\n",
    "        j += 1\n",
    "\n",
    "interactions_pos_neg_df = pd.concat(\n",
    "    [interactions_pos_neg_df, pd.DataFrame(negative_interactions, columns=['user_id', 'item_id', 'interacted'])])\n",
    "interactions_pos_neg_df = interactions_pos_neg_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-equipment",
   "metadata": {},
   "source": [
    "**Task 1.** Code the GMFModel as described in the He et al., Neural Collaborative Filtering paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMFModel(nn.Module):\n",
    "    def __init__(self, n_items, n_users, embedding_dim, seed):\n",
    "        \"\"\"\n",
    "        :param int n_items: Number of items.\n",
    "        :param int n_users: Number of users.\n",
    "        :param int embedding_dim: Dimension of the user and item embeddings.\n",
    "        :param int seed: Seed for the random number generator.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        ########################\n",
    "        # Write your code here #\n",
    "        ########################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param torch.Tensor x: Tensor of size batch_size x 2. Every row contains two values - user id in the first column \n",
    "            and item id in the second column.\n",
    "        \"\"\"\n",
    "        ########################\n",
    "        # Write your code here #\n",
    "        ########################\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic test of the model\n",
    "\n",
    "net = GMFModel(\n",
    "    n_items=n_items, \n",
    "    n_users=n_users, \n",
    "    embedding_dim=8, \n",
    "    seed=seed)\n",
    "\n",
    "display(interactions_pos_neg_df[:10])\n",
    "display(interactions_pos_neg_df[-10:])\n",
    "print()\n",
    "result = net(torch.from_numpy(interactions_pos_neg_df[:10][['user_id', 'item_id']].values).long())\n",
    "print(result)\n",
    "assert torch.allclose(result, torch.tensor([[0.5502], [0.8116], [0.3320], [0.6224], [0.9059], [0.7631], [0.3816], [0.8437], \n",
    "                                            [0.5892], [0.6404]]), atol=1e-04)\n",
    "print()\n",
    "result = net(torch.from_numpy(interactions_pos_neg_df[-10:][['user_id', 'item_id']].values).long())\n",
    "print(result)\n",
    "assert torch.allclose(result, torch.tensor([[0.3559], [0.5642], [0.3651], [0.4302], [0.4346], [0.3870], [0.5180], [0.4654], \n",
    "                                            [0.5181], [0.4620]]), atol=1e-04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-hebrew",
   "metadata": {},
   "source": [
    "**Task 2.** Code the MLPModel as described in the He et al., Neural Collaborative Filtering paper. Use two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, n_items, n_users, embedding_dim, hidden_dim, seed):\n",
    "        \"\"\"\n",
    "        :param int n_items: Number of items.\n",
    "        :param int n_users: Number of users.\n",
    "        :param int embedding_dim: Dimension of the user and item embeddings.\n",
    "        :param int hidden_dim: Dimension of the first hidden layer.\n",
    "        :param int seed: Seed for the random number generator.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        ########################\n",
    "        # Write your code here #\n",
    "        ########################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param torch.Tensor x: Tensor of size batch_size x 2. Every row contains two values - user id in the first column \n",
    "            and item id in the second column.\n",
    "        \"\"\"\n",
    "        ########################\n",
    "        # Write your code here #\n",
    "        ########################\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic test of the model\n",
    "\n",
    "net = MLPModel(\n",
    "    n_items=n_items, \n",
    "    n_users=n_users, \n",
    "    embedding_dim=8, \n",
    "    hidden_dim=16,\n",
    "    seed=seed)\n",
    "\n",
    "display(interactions_pos_neg_df[:10])\n",
    "display(interactions_pos_neg_df[-10:])\n",
    "print()\n",
    "result = net(torch.from_numpy(interactions_pos_neg_df[:10][['user_id', 'item_id']].values).long())\n",
    "print(result)\n",
    "assert torch.allclose(result, torch.tensor([[0.5387], [0.5004], [0.4918], [0.4821], [0.5004], [0.4994], [0.5055], [0.5166],\n",
    "                                            [0.4612], [0.4645]]), atol=1e-04)\n",
    "print()\n",
    "result = net(torch.from_numpy(interactions_pos_neg_df[-10:][['user_id', 'item_id']].values).long())\n",
    "print(result)\n",
    "assert torch.allclose(result, torch.tensor([[0.5154], [0.5140], [0.5110], [0.4910], [0.4899], [0.5082], [0.4688], [0.5164],\n",
    "                                            [0.4983], [0.5035]]), atol=1e-04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-wallace",
   "metadata": {},
   "source": [
    "**Task 3.** Code the NeuMFModel as described in the He et al., Neural Collaborative Filtering paper. Use two hidden layers in the MLP part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuMFModel(nn.Module):\n",
    "    def __init__(self, n_items, n_users, gmf_embedding_dim, mlp_embedding_dim, hidden_dim, seed):\n",
    "        \"\"\"\n",
    "        :param int n_items: Number of items.\n",
    "        :param int n_users: Number of users.\n",
    "        :param int embedding_dim: Dimension of the user and item embeddings.\n",
    "        :param int hidden_dim: Dimension of the first hidden layer of the MLP part of the network.\n",
    "        :param int seed: Seed for the random number generator.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        ########################\n",
    "        # Write your code here #\n",
    "        ########################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param torch.Tensor x: Tensor of size batch_size x 2. Every row contains two values - user id in the first column \n",
    "            and item id in the second column.\n",
    "        \"\"\"\n",
    "        ########################\n",
    "        # Write your code here #\n",
    "        ########################\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic test of the model\n",
    "\n",
    "net = NeuMFModel(\n",
    "    n_items=n_items, \n",
    "    n_users=n_users, \n",
    "    gmf_embedding_dim=8, \n",
    "    mlp_embedding_dim=8, \n",
    "    hidden_dim=16,\n",
    "    seed=seed)\n",
    "\n",
    "display(interactions_pos_neg_df[:10])\n",
    "display(interactions_pos_neg_df[-10:])\n",
    "print()\n",
    "result = net(torch.from_numpy(interactions_pos_neg_df[:10][['user_id', 'item_id']].values).long())\n",
    "print(result)\n",
    "assert torch.allclose(result, torch.tensor([[0.5841], [0.3518], [0.5470], [0.6893], [0.4029], [0.5030], [0.4145], [0.3592],\n",
    "                                            [0.5345], [0.3968]]), atol=1e-04)\n",
    "print()\n",
    "result = net(torch.from_numpy(interactions_pos_neg_df[-10:][['user_id', 'item_id']].values).long())\n",
    "print(result)\n",
    "assert torch.allclose(result, torch.tensor([[0.4201], [0.5565], [0.6616], [0.3611], [0.6450], [0.4355], [0.5193], [0.4625],\n",
    "                                            [0.5642], [0.6547]]), atol=1e-04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-oxide",
   "metadata": {},
   "source": [
    "**Task 4.** Code the train_ncf method which takes a neural collaborative filtering model and performs mini-batch gradient descent to train the model. The interface of the method should be as follows:\n",
    "\n",
    "    train_ncf(net, interactions_df, n_epochs, batch_size, lr, weight_decay, seed)\n",
    "\n",
    "where net is an initialized neural network which takes a batch of pairs (user_id, item_id) as input and returns a tensor of scores for every pair, interactions_df is the interactions DataFrame with the following columns user_id, item_id, interacted, n_epochs is the number of epochs (i.e. the number of times the network should be trained on the entire dataset), batch_size is the mini-batch size on which a single loss should be calculated and backpropagated, lr is the learning rate for SGD, weight_decay is the L2 penalty weight, seed is the seed for the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ncf(net, interactions_df, n_epochs, batch_size, lr, weight_decay, seed):\n",
    "    \"\"\"\n",
    "    :param type net: Trained neural network which takes a batch of pairs (user_id, item_id) \n",
    "        as input and returns a tensor of scores for every pair.\n",
    "    :param pd.DataFrame interactions_df: Interactions DataFrame with the following columns user_id, item_id, interacted.\n",
    "    :param int n_epochs: Number of epochs.\n",
    "    :param int batch_size: Batch size.\n",
    "    :param float lr: Learning rate.\n",
    "    :param float weight_decay: L2 penalty weight.\n",
    "    :param int seed: Seed for the random number generator.\n",
    "    \"\"\"\n",
    "    liveloss = PlotLosses()\n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "\n",
    "    # Initialize the Adam optimizer\n",
    "\n",
    "    ########################\n",
    "    # Write your code here #\n",
    "    ########################\n",
    "\n",
    "    \n",
    "    # Train the model \n",
    "    # Write an external loop over epochs and an internal loop over batches.\n",
    "    # For every epoch first shuffle the ids of interactions_df using rng.permutation and then select\n",
    "    # the elements of interactions_df through those shuffled ids.\n",
    "    # Remember that the dataset size may be indivisible by batch_size.\n",
    "    # Make sure to also use the last incomplete batch for training.\n",
    "    # After generating model results clip them with clip(0.000001, 0.999999) to avoid the error of trying to\n",
    "    # calculate log(0) in the loss function\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_total_loss = 0.0\n",
    "        ########################\n",
    "        # Write your code here #\n",
    "        ########################\n",
    "\n",
    "\n",
    "\n",
    "        # Print epoch losses\n",
    "\n",
    "        epoch_avg_loss = epoch_total_loss / len(interaction_ids)\n",
    "\n",
    "        if epoch >= 0:\n",
    "            # A bound on epoch prevents showing extremely high losses in the first epochs\n",
    "            logs = {}\n",
    "            logs['loss'] = epoch_avg_loss\n",
    "            liveloss.update(logs)\n",
    "            liveloss.send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the method\n",
    "\n",
    "net = GMFModel(\n",
    "    n_items=n_items, \n",
    "    n_users=n_users, \n",
    "    embedding_dim=8, \n",
    "    seed=seed)\n",
    "\n",
    "train_ncf(net=net, interactions_df=interactions_pos_neg_df, n_epochs=10, batch_size=64, lr=0.01, weight_decay=0.001, seed=6789)\n",
    "\n",
    "display(interactions_pos_neg_df[:10])\n",
    "display(interactions_pos_neg_df[-10:])\n",
    "print()\n",
    "result = net(torch.from_numpy(interactions_pos_neg_df[:10][['user_id', 'item_id']].values).long())\n",
    "print(result)\n",
    "assert torch.allclose(result, torch.tensor([[0.9965], [1.0000], [0.9643], [1.0000], [0.8465], [0.9877], [0.9028], [0.9575],\n",
    "                                            [0.9999], [0.9792]]), atol=1e-04)\n",
    "print()\n",
    "result = net(torch.from_numpy(interactions_pos_neg_df[-10:][['user_id', 'item_id']].values).long())\n",
    "print(result)\n",
    "assert torch.allclose(result, torch.tensor([[9.2855e-04], [1.2987e-04], [8.2114e-05], [3.9465e-03], [1.1282e-07], [1.3399e-02],\n",
    "                                            [6.6296e-05], [2.0571e-02], [3.7681e-03], [2.4262e-04]]), atol=1e-04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-jerusalem",
   "metadata": {},
   "source": [
    "**Task 5.** Code the recommend method which takes user_id, interactions_df and a trained neural collaborative filtering model as input and returns the best recommendation (item_id) and its score for the given user based on scores calculated by the network for every pair user_id, item_id. Remember to map the user id with user_id_mapping at the beginning and item_id with item_id_reverse_mapping before returning item_id. Do not include films the user has already watched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_id, interactions_df, net):\n",
    "    \"\"\"\n",
    "    :param int user_id: User id (as in the original dataset before the indices shift).\n",
    "    :param pd.DataFrame interactions_df: Interactions DataFrame with the following columns user_id, item_id.\n",
    "    :param type net: Trained neural network which takes a batch of pairs (user_id, item_id) \n",
    "        as input and returns a tensor of scores for every pair.\n",
    "    \"\"\"\n",
    "    ########################\n",
    "    # Write your code here #\n",
    "    ########################\n",
    "\n",
    "\n",
    "\n",
    "# Test\n",
    "\n",
    "user_id = user_id_reverse_mapping[534]\n",
    "item_id, score = recommend(user_id, interactions_df, net)\n",
    "title = ml_movies_df.loc[ml_movies_df['item_id'] == item_id, 'title'].iloc[0]\n",
    "\n",
    "print(\"Best item id: {}\".format(item_id))\n",
    "print(\"Best item title: {}\".format(title))\n",
    "print(\"Best item score: {}\".format(score))\n",
    "assert item_id == 1500\n",
    "assert np.abs(score - 0.9996901750564575) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-prediction",
   "metadata": {},
   "source": [
    "# Generalized Matrix Factorization recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "\n",
    "from recommenders.recommender import Recommender\n",
    "\n",
    "\n",
    "class GMFRecommender(Recommender):\n",
    "    \"\"\"\n",
    "    General Matrix Factorization recommender as described in:\n",
    "    - He X., Liao L., Zhang H., Nie L., Hu X., Chua T., Neural Collaborative Filtering, WWW Conference, 2017\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seed=6789, n_neg_per_pos=5, print_type=None, **params):\n",
    "        super().__init__()\n",
    "        self.recommender_df = pd.DataFrame(columns=['user_id', 'item_id', 'score'])\n",
    "        self.interactions_df = None\n",
    "        self.item_id_mapping = None\n",
    "        self.user_id_mapping = None\n",
    "        self.item_id_reverse_mapping = None\n",
    "        self.user_id_reverse_mapping = None\n",
    "        self.r = None\n",
    "        self.most_popular_items = None\n",
    "        \n",
    "        self.nn_model = None\n",
    "        self.optimizer = None\n",
    "        \n",
    "        self.n_neg_per_pos = n_neg_per_pos\n",
    "        if 'n_epochs' in params:  # number of epochs (each epoch goes through the entire training set)\n",
    "            self.n_epochs = params['n_epochs']\n",
    "        else:\n",
    "            self.n_epochs = 10\n",
    "        if 'lr' in params:  # learning rate\n",
    "            self.lr = params['lr']\n",
    "        else:\n",
    "            self.lr = 0.01\n",
    "        if 'weight_decay' in params:  # weight decay (L2 regularization)\n",
    "            self.weight_decay = params['weight_decay']\n",
    "        else:\n",
    "            self.weight_decay = 0.001\n",
    "        if 'embedding_dim' in params:\n",
    "            self.embedding_dim = params['embedding_dim']\n",
    "        else:\n",
    "            self.embedding_dim = 4\n",
    "        if 'batch_size' in params:\n",
    "            self.batch_size = params['batch_size']\n",
    "        else:\n",
    "            self.batch_size = 64\n",
    "        if 'device' in params:\n",
    "            self.device = params['device']\n",
    "        else:\n",
    "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        if 'should_recommend_already_bought' in params:\n",
    "            self.should_recommend_already_bought = params['should_recommend_already_bought']\n",
    "        else:\n",
    "            self.should_recommend_already_bought = False\n",
    "        \n",
    "        if 'train' in params:\n",
    "            self.train = params['train']\n",
    "        else:\n",
    "            self.train = False\n",
    "        self.validation_set_size = 0.2\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.rng = np.random.RandomState(seed=seed)\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        if 'should_save_model' in params:\n",
    "            self.should_save_model = params['should_save_model']\n",
    "        self.print_type = print_type\n",
    "\n",
    "    def fit(self, interactions_df, users_df, items_df):\n",
    "        \"\"\"\n",
    "        Training of the recommender.\n",
    "\n",
    "        :param pd.DataFrame interactions_df: DataFrame with recorded interactions between users and items\n",
    "            defined by user_id, item_id and features of the interaction.\n",
    "        :param pd.DataFrame users_df: DataFrame with users and their features defined by\n",
    "            user_id and the user feature columns.\n",
    "        :param pd.DataFrame items_df: DataFrame with items and their features defined\n",
    "            by item_id and the item feature columns.\n",
    "        \"\"\"\n",
    "\n",
    "        del users_df, items_df\n",
    "\n",
    "        # Shift item ids and user ids so that they are consecutive\n",
    "\n",
    "        unique_item_ids = interactions_df['item_id'].unique()\n",
    "        self.item_id_mapping = dict(zip(unique_item_ids, list(range(len(unique_item_ids)))))\n",
    "        self.item_id_reverse_mapping = dict(zip(list(range(len(unique_item_ids))), unique_item_ids))\n",
    "        unique_user_ids = interactions_df['user_id'].unique()\n",
    "        self.user_id_mapping = dict(zip(unique_user_ids, list(range(len(unique_user_ids)))))\n",
    "        self.user_id_reverse_mapping = dict(zip(list(range(len(unique_user_ids))), unique_user_ids))\n",
    "\n",
    "        interactions_df = interactions_df.copy()\n",
    "        interactions_df['item_id'] = interactions_df['item_id'].map(self.item_id_mapping)\n",
    "        interactions_df['user_id'] = interactions_df['user_id'].map(self.user_id_mapping)\n",
    "\n",
    "        # Get the number of items and users\n",
    "\n",
    "        self.interactions_df = interactions_df.copy()\n",
    "        n_users = np.max(interactions_df['user_id']) + 1\n",
    "        n_items = np.max(interactions_df['item_id']) + 1\n",
    "\n",
    "        # Get the user-item interaction matrix (mapping to int is necessary because of how iterrows works)\n",
    "        r = np.zeros(shape=(n_users, n_items))\n",
    "        for idx, interaction in interactions_df.iterrows():\n",
    "            r[int(interaction['user_id'])][int(interaction['item_id'])] = 1\n",
    "\n",
    "        self.r = r\n",
    "        \n",
    "        # Indicate positive interactions\n",
    "        \n",
    "        interactions_df.loc[:, 'interacted'] = 1\n",
    "\n",
    "        # Generate negative interactions\n",
    "        negative_interactions = []\n",
    "\n",
    "        i = 0\n",
    "        while i < self.n_neg_per_pos * len(interactions_df):\n",
    "            sample_size = 1000\n",
    "            user_ids = self.rng.choice(np.arange(n_users), size=sample_size)\n",
    "            item_ids = self.rng.choice(np.arange(n_items), size=sample_size)\n",
    "\n",
    "            j = 0\n",
    "            while j < sample_size and i < self.n_neg_per_pos * len(interactions_df):\n",
    "                if r[user_ids[j]][item_ids[j]] == 0:\n",
    "                    negative_interactions.append([user_ids[j], item_ids[j], 0])\n",
    "                    i += 1\n",
    "                j += 1\n",
    "        \n",
    "        interactions_df = pd.concat(\n",
    "            [interactions_df, pd.DataFrame(negative_interactions, columns=['user_id', 'item_id', 'interacted'])])\n",
    "        interactions_df = interactions_df.reset_index(drop=True)\n",
    "        \n",
    "        # Initialize losses and loss visualization\n",
    "        \n",
    "        if self.print_type is not None and self.print_type == 'live':\n",
    "            liveloss = PlotLosses()\n",
    "\n",
    "        training_losses = deque(maxlen=50)\n",
    "        training_avg_losses = []\n",
    "        training_epoch_losses = []\n",
    "        validation_losses = deque(maxlen=50)\n",
    "        validation_avg_losses = []\n",
    "        validation_epoch_losses = []\n",
    "        last_training_total_loss = 0.0\n",
    "        last_validation_total_loss = 0.0\n",
    "        \n",
    "        # Initialize the network\n",
    "        \n",
    "        self.nn_model = GMFModel(n_items, n_users, self.embedding_dim, self.seed)\n",
    "        self.nn_model.train()\n",
    "        self.nn_model.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.nn_model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        \n",
    "        # Split the data\n",
    "        \n",
    "        if self.train:\n",
    "            interaction_ids = self.rng.permutation(len(interactions_df))\n",
    "            train_validation_slice_idx = int(len(interactions_df) * (1 - self.validation_set_size))\n",
    "            training_ids = interaction_ids[:train_validation_slice_idx]\n",
    "            validation_ids = interaction_ids[train_validation_slice_idx:]\n",
    "        else:\n",
    "            interaction_ids = self.rng.permutation(len(interactions_df))\n",
    "            training_ids = interaction_ids\n",
    "            validation_ids = []\n",
    "        \n",
    "        # Train the model\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            if self.print_type is not None and self.print_type == 'live':\n",
    "                logs = {}\n",
    "                \n",
    "            # Train\n",
    "            \n",
    "            training_losses.clear()\n",
    "            training_total_loss = 0.0\n",
    "            \n",
    "            self.rng.shuffle(training_ids)\n",
    "            \n",
    "            batch_idx = 0\n",
    "            n_batches = int(np.ceil(len(training_ids) / self.batch_size))\n",
    "            \n",
    "            for batch_idx in range(n_batches):\n",
    "                \n",
    "                batch_ids = training_ids[(batch_idx * self.batch_size):((batch_idx + 1) * self.batch_size)]\n",
    "                \n",
    "                batch = interactions_df.loc[batch_ids]\n",
    "                batch_input = torch.from_numpy(batch.loc[:, ['user_id', 'item_id']].values).long().to(self.device)\n",
    "                y_target = torch.from_numpy(batch.loc[:, ['interacted']].values).float().to(self.device)\n",
    "                \n",
    "                # Create responses\n",
    "\n",
    "                y = self.nn_model(batch_input).clip(0.000001, 0.999999)\n",
    "\n",
    "                # Define loss and backpropagate\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = -(y_target * y.log() + (1 - y_target) * (1 - y).log()).sum()\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                training_total_loss += loss.item()\n",
    "                \n",
    "                if self.print_type is not None and self.print_type == 'text':\n",
    "                    print(\"\\rEpoch: {}\\tBatch: {}\\tLast epoch - avg training loss: {:.2f} avg validation loss: {:.2f} loss: {}\".format(\n",
    "                        epoch, batch_idx, last_training_total_loss, last_validation_total_loss, loss), end=\"\")\n",
    "                \n",
    "                training_losses.append(loss.item())\n",
    "                training_avg_losses.append(np.mean(training_losses))\n",
    "                \n",
    "            # Validate\n",
    "            \n",
    "            if self.train:\n",
    "\n",
    "                validation_total_loss = 0.0\n",
    "\n",
    "                batch = interactions_df.loc[validation_ids]\n",
    "                batch_input = torch.from_numpy(batch.loc[:, ['user_id', 'item_id']].values).long().to(self.device)\n",
    "                y_target = torch.from_numpy(batch.loc[:, ['interacted']].values).float().to(self.device)\n",
    "\n",
    "                # Create responses\n",
    "\n",
    "                y = self.nn_model(batch_input).clip(0.000001, 0.999999)\n",
    "\n",
    "                # Calculate validation loss\n",
    "\n",
    "                loss = -(y_target * y.log() + (1 - y_target) * (1 - y).log()).sum()\n",
    "                validation_total_loss += loss.item()\n",
    "                \n",
    "            # Save and print epoch losses\n",
    "            \n",
    "            training_last_avg_loss = training_total_loss / len(training_ids)\n",
    "            \n",
    "            if self.train:\n",
    "                validation_last_avg_loss = validation_total_loss / len(validation_ids)\n",
    "\n",
    "            if self.print_type is not None and self.print_type == 'live' and epoch >= 0:\n",
    "                # A bound on epoch prevents showing extremely high losses in the first epochs\n",
    "                logs['loss'] = training_last_avg_loss\n",
    "                if self.train:\n",
    "                    logs['val_loss'] = validation_last_avg_loss\n",
    "                liveloss.update(logs)\n",
    "                liveloss.send()\n",
    "\n",
    "        # Find the most popular items for the cold start problem\n",
    "\n",
    "        offers_count = interactions_df.loc[:, ['item_id', 'user_id']].groupby(by='item_id').count()\n",
    "        offers_count = offers_count.sort_values('user_id', ascending=False)\n",
    "        self.most_popular_items = offers_count.index\n",
    "\n",
    "    def recommend(self, users_df, items_df, n_recommendations=1):\n",
    "        \"\"\"\n",
    "        Serving of recommendations. Scores items in items_df for each user in users_df and returns\n",
    "        top n_recommendations for each user.\n",
    "\n",
    "        :param pd.DataFrame users_df: DataFrame with users and their features for which\n",
    "            recommendations should be generated.\n",
    "        :param pd.DataFrame items_df: DataFrame with items and their features which should be scored.\n",
    "        :param int n_recommendations: Number of recommendations to be returned for each user.\n",
    "        :return: DataFrame with user_id, item_id and score as columns returning n_recommendations top recommendations\n",
    "            for each user.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        # Clean previous recommendations (iloc could be used alternatively)\n",
    "        self.recommender_df = self.recommender_df[:0]\n",
    "\n",
    "        # Handle users not in the training data\n",
    "\n",
    "        # Map item ids\n",
    "\n",
    "        items_df = items_df.copy()\n",
    "        items_df = items_df.loc[items_df['item_id'].isin(self.item_id_mapping)]\n",
    "        items_df['item_id'] = items_df['item_id'].map(self.item_id_mapping)\n",
    "\n",
    "        # Generate recommendations\n",
    "\n",
    "        for idx, user in users_df.iterrows():\n",
    "            recommendations = []\n",
    "\n",
    "            user_id = user['user_id']\n",
    "\n",
    "            if user_id in self.user_id_mapping:\n",
    "                \n",
    "                mapped_user_id = self.user_id_mapping[user_id]\n",
    "                \n",
    "                ids_list = items_df['item_id'].tolist()\n",
    "                id_to_pos = np.array([0]*len(ids_list))\n",
    "                for k in range(len(ids_list)):\n",
    "                    id_to_pos[ids_list[k]] = k\n",
    "                \n",
    "                net_input = torch.tensor(list(zip([mapped_user_id]*len(ids_list), ids_list))).to(self.device)\n",
    "                \n",
    "                scores = self.nn_model(net_input).flatten().detach().cpu().numpy()\n",
    "                \n",
    "                # Choose n recommendations based on highest scores\n",
    "                if not self.should_recommend_already_bought:\n",
    "                    x_list = self.interactions_df.loc[\n",
    "                        self.interactions_df['user_id'] == mapped_user_id]['item_id'].tolist()\n",
    "                    scores[id_to_pos[x_list]] = -np.inf\n",
    "\n",
    "                chosen_pos = np.argsort(-scores)[:n_recommendations]\n",
    "\n",
    "                for item_pos in chosen_pos:\n",
    "                    recommendations.append(\n",
    "                        {\n",
    "                            'user_id': self.user_id_reverse_mapping[mapped_user_id],\n",
    "                            'item_id': self.item_id_reverse_mapping[ids_list[item_pos]],\n",
    "                            'score': scores[item_pos]\n",
    "                        }\n",
    "                    )\n",
    "            else:  # For new users recommend most popular items\n",
    "                for i in range(n_recommendations):\n",
    "                    recommendations.append(\n",
    "                        {\n",
    "                            'user_id': user['user_id'],\n",
    "                            'item_id': self.item_id_reverse_mapping[self.most_popular_items[i]],\n",
    "                            'score': 1.0\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            user_recommendations = pd.DataFrame(recommendations)\n",
    "\n",
    "            self.recommender_df = pd.concat([self.recommender_df, user_recommendations])\n",
    "\n",
    "        return self.recommender_df\n",
    "    \n",
    "    def get_user_repr(self, user_id):\n",
    "        mapped_user_id = self.user_id_mapping[user_id]\n",
    "        return self.nn_model.user_embedding(torch.tensor(mapped_user_id).to(self.device)).detach().cpu().numpy()\n",
    "    \n",
    "    def get_item_repr(self, item_id):\n",
    "        mapped_item_id = self.item_id_mapping[item_id]\n",
    "        return self.nn_model.item_embedding(torch.tensor(mapped_item_id).to(self.device)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-offering",
   "metadata": {},
   "source": [
    "## Quick test of the recommender (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_recommender = GMFRecommender(print_type='live', n_neg_per_pos=10, batch_size=16, \n",
    "                                 embedding_dim=6, lr=0.001, weight_decay=0.0001, n_epochs=30, seed=1, train=True)\n",
    "gmf_recommender.fit(ml_ratings_df, None, ml_movies_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-messaging",
   "metadata": {},
   "source": [
    "## Quick test of the recommender (recommending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = gmf_recommender.recommend(pd.DataFrame([[1], [4], [6]], columns=['user_id']), ml_movies_df, 10)\n",
    "\n",
    "recommendations = pd.merge(recommendations, ml_movies_df, on='item_id', how='left')\n",
    "print(\"Recommendations\")\n",
    "display(HTML(recommendations.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-barcelona",
   "metadata": {},
   "source": [
    "## User and item representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-detective",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_id = 1\n",
    "user_repr = gmf_recommender.get_user_repr(user_id=user_id)\n",
    "print(\"User id={}\".format(user_id))\n",
    "print(user_repr)\n",
    "print()\n",
    "\n",
    "print(\"User watched\")\n",
    "print(ml_df.loc[ml_df['user_id'] == user_id, 'title'].tolist())\n",
    "print()\n",
    "\n",
    "print('User history item representations')\n",
    "for item_id in ml_df.loc[ml_df['user_id'] == user_id, 'item_id'].tolist():\n",
    "    item_repr = gmf_recommender.get_item_repr(item_id=item_id)\n",
    "    print(\"Item id = {}\\titem title = {}\".format(\n",
    "        item_id, ml_movies_df.loc[ml_movies_df['item_id'] == item_id, 'title'].iloc[0]))\n",
    "    print(item_repr)\n",
    "    scalar_product = np.dot(user_repr, item_repr)\n",
    "    print(\"Scalar product={:.6f}\".format(scalar_product))\n",
    "    score = gmf_recommender.nn_model(\n",
    "        torch.tensor([[gmf_recommender.user_id_mapping[user_id], \n",
    "                       gmf_recommender.item_id_mapping[item_id]]]).to(gmf_recommender.device)).flatten().detach().cpu().item()\n",
    "    print(\"Score={:.6f}\".format(score))\n",
    "    print()\n",
    "\n",
    "print(\"===============\")\n",
    "    \n",
    "item_id = 145\n",
    "item_repr = gmf_recommender.get_item_repr(item_id=item_id)\n",
    "print(\"Item id = {}\\titem title = {}\".format(item_id, ml_movies_df.loc[ml_movies_df['item_id'] == item_id, 'title'].iloc[0]))\n",
    "print(item_repr)\n",
    "score = np.dot(user_repr, item_repr)\n",
    "print(\"Scalar product={:.6f}\".format(score))\n",
    "score = gmf_recommender.nn_model(\n",
    "    torch.tensor([[gmf_recommender.user_id_mapping[user_id], \n",
    "                   gmf_recommender.item_id_mapping[item_id]]]).to(gmf_recommender.device)).flatten().detach().cpu().item()\n",
    "print(\"Score={:.6f}\".format(score))\n",
    "print()\n",
    "\n",
    "item_id = 171\n",
    "item_repr = gmf_recommender.get_item_repr(item_id=item_id)\n",
    "print(\"Item id = {}\\titem title = {}\".format(item_id, ml_movies_df.loc[ml_movies_df['item_id'] == item_id, 'title'].iloc[0]))\n",
    "print(item_repr)\n",
    "score = np.dot(user_repr, item_repr)\n",
    "print(\"Scalar product={:.6f}\".format(score))\n",
    "score = gmf_recommender.nn_model(\n",
    "    torch.tensor([[gmf_recommender.user_id_mapping[user_id], \n",
    "                   gmf_recommender.item_id_mapping[item_id]]]).to(gmf_recommender.device)).flatten().detach().cpu().item()\n",
    "print(\"Score={:.6f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-negative",
   "metadata": {},
   "source": [
    "# Training-test split evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_and_testing.testing import evaluate_train_test_split_implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_recommender = GMFRecommender(n_neg_per_pos=10, batch_size=16, \n",
    "                                 embedding_dim=6, lr=0.001, weight_decay=0.0001, n_epochs=25)\n",
    "\n",
    "gmf_tts_results = [['GMFRecommender'] + list(evaluate_train_test_split_implicit(\n",
    "    gmf_recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df))]\n",
    "\n",
    "gmf_tts_results = pd.DataFrame(\n",
    "    gmf_tts_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(gmf_tts_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.netflix_recommender import NetflixRecommender\n",
    "\n",
    "netflix_recommender = NetflixRecommender(n_epochs=150)\n",
    "\n",
    "netflix_tts_results = [['NetflixRecommender'] + list(evaluate_train_test_split_implicit(\n",
    "    netflix_recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df))]\n",
    "\n",
    "netflix_tts_results = pd.DataFrame(\n",
    "    netflix_tts_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(netflix_tts_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.amazon_recommender import AmazonRecommender\n",
    "\n",
    "amazon_recommender = AmazonRecommender()\n",
    "\n",
    "amazon_tts_results = [['AmazonRecommender'] + list(evaluate_train_test_split_implicit(\n",
    "    amazon_recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df))]\n",
    "\n",
    "amazon_tts_results = pd.DataFrame(\n",
    "    amazon_tts_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(amazon_tts_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.tfidf_recommender import TFIDFRecommender\n",
    "\n",
    "tfidf_recommender = TFIDFRecommender()\n",
    "\n",
    "tfidf_tts_results = [['TFIDFRecommender'] + list(evaluate_train_test_split_implicit(\n",
    "    tfidf_recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df))]\n",
    "\n",
    "tfidf_tts_results = pd.DataFrame(\n",
    "    tfidf_tts_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(tfidf_tts_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_results = pd.concat([gmf_tts_results, netflix_tts_results, amazon_tts_results, tfidf_tts_results]).reset_index(drop=True)\n",
    "display(HTML(tts_results.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-harassment",
   "metadata": {},
   "source": [
    "# Leave-one-out evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_and_testing.testing import evaluate_leave_one_out_implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-resistance",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_recommender = GMFRecommender(n_neg_per_pos=10, batch_size=16, \n",
    "                                 embedding_dim=6, lr=0.001, weight_decay=0.0001, n_epochs=25)\n",
    "\n",
    "gmf_loo_results = [['GMFRecommender'] + list(evaluate_leave_one_out_implicit(\n",
    "    gmf_recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df, max_evals=300, seed=6789))]\n",
    "\n",
    "gmf_loo_results = pd.DataFrame(\n",
    "    gmf_loo_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(gmf_loo_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_recommender = NetflixRecommender(n_epochs=150)\n",
    "\n",
    "netflix_loo_results = [['NetflixRecommender'] + list(evaluate_leave_one_out_implicit(\n",
    "    netflix_recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df, max_evals=300, seed=6789))]\n",
    "\n",
    "netflix_loo_results = pd.DataFrame(\n",
    "    netflix_loo_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(netflix_loo_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.amazon_recommender import AmazonRecommender\n",
    "\n",
    "amazon_recommender = AmazonRecommender()\n",
    "\n",
    "amazon_loo_results = [['AmazonRecommender'] + list(evaluate_leave_one_out_implicit(\n",
    "    amazon_recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df, max_evals=300, seed=6789))]\n",
    "\n",
    "amazon_loo_results = pd.DataFrame(\n",
    "    amazon_loo_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(amazon_loo_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_recommender = TFIDFRecommender()\n",
    "\n",
    "tfidf_loo_results = [['TFIDFRecommender'] + list(evaluate_leave_one_out_implicit(\n",
    "    tfidf_recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df, max_evals=300, seed=6789))]\n",
    "\n",
    "tfidf_loo_results = pd.DataFrame(\n",
    "    tfidf_loo_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(tfidf_loo_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "loo_results = pd.concat([gmf_loo_results, netflix_loo_results, amazon_loo_results, tfidf_loo_results]).reset_index(drop=True)\n",
    "display(HTML(loo_results.to_html()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
