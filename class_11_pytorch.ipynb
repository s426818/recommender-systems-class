{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "approximate-classic",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "Here's your best friend when working with PyTorch: https://pytorch.org/docs/stable/index.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spread-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D, art3d\n",
    "from matplotlib.patches import Circle, Ellipse\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "from IPython.display import Markdown, display, HTML\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Fix the dying kernel problem (only a problem in some installations - you can remove it, if it works without it)\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-street",
   "metadata": {},
   "source": [
    "## PyTorch basic operations tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-niger",
   "metadata": {},
   "source": [
    "**Task 1.** Calculate the sigmoid (logistic) function on every element of the following array [0.3, 1.2, -1.4, 0.2, -0.1, 0.1, 0.8, -0.25] and print the last 5 elements. Use only tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "agreed-single",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5498, 0.4750, 0.5250, 0.6900, 0.4378])\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "array_a = torch.tensor([0.3, 1.2, -1.4, 0.2, -0.1, 0.1, 0.8, -0.25])\n",
    "sigmoid_on_array_a = torch.sigmoid(array_a)\n",
    "last_5_sigmoid_on_array_a = sigmoid_on_array_a[((len(sigmoid_on_array_a))-5):]\n",
    "print(last_5_sigmoid_on_array_a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-catch",
   "metadata": {},
   "source": [
    "**Task 2.** Calculate the dot product of the following two vectors:<br/>\n",
    "$x = [3, 1, 4, 2, 6, 1, 4, 8]$<br/>\n",
    "$y = [5, 2, 3, 12, 2, 4, 17, 9]$<br/>\n",
    "a) by using element-wise mutliplication and torch.sum,<br/>\n",
    "b) by using torch.dot,<br/>\n",
    "b) by using torch.matmul and transposition (x.T)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forbidden-journalism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n",
      "209\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "array_x = torch.tensor([3,1,4,2,6,1,4,8])\n",
    "array_y = torch.tensor([5,2,3,12,2,4,17,9])\n",
    "\n",
    "dot_product_x_y_a = array_x * array_y\n",
    "sum_dot_product_x_y_a = torch.sum(dot_product_x_y_a).item()\n",
    "print(sum_dot_product_x_y_a)\n",
    "\n",
    "dot_product_x_y_b = torch.dot(array_x, array_y).item()\n",
    "print(dot_product_x_y_b)\n",
    "\n",
    "dot_product_x_y_c = torch.matmul(array_x, array_y.T).item()\n",
    "print(dot_product_x_y_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-amber",
   "metadata": {},
   "source": [
    "**Task 3.** Calculate the following expression<br/>\n",
    "$$\\frac{1}{1 + e^{-x_0 \\theta_0 - \\ldots - x_9 \\theta_9 - \\theta_{10}}}$$\n",
    "for<br/>\n",
    "$x = [1.2, 2.3, 3.4, -0.7, 4.2, 2.7, -0.5, 1.4, -3.3, 0.2]$<br/>\n",
    "$\\theta = [1.7, 0.33, -2.12, -1.73, 2.9, -5.8, -0.9, 12.11, 3.43, -0.5, -1.65]$<br/>\n",
    "and print the result. Use only tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "falling-holder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08762359797780898\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "array_x_3 = torch.tensor([1.2,2.3,3.4, -0.7,4.2,2.7,-0.5,1.4,-3.3,0.2])\n",
    "#array_x_3 = array_x_3.add(1)\n",
    "array_theta_3 = torch.tensor([1.7,0.33,-2.12,-1.73,2.9,-5.8,-0.9,12.11,3.43,-0.5,-1.65])\n",
    "exponent_3 = -(torch.matmul(array_x_3, array_theta_3[:(len(array_theta_3)-1)].T).item())\n",
    "exponent_3 -= array_theta_3[len(array_theta_3)-1]\n",
    "value_3 = 1 / (1 + torch.exp(exponent_3)).item()\n",
    "print(value_3)\n",
    "\n",
    "'''\n",
    "poprawne\n",
    "x = torch.tensor([1.2, 2.3, 3.4, -0.7, 4.2, 2.7, -0.5, 1.4, -3.3, 0.2])\n",
    "theta = torch.tensor([1.7, 0.33, -2.12, -1.73, 2.9, -5.8, -0.9, 12.11, 3.43, -0.5, -1.65])\n",
    "print((1 / (1 + torch.exp(-torch.sum(x * theta[:10]) - theta[10]))).item())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-vector",
   "metadata": {},
   "source": [
    "# Tensor gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-serial",
   "metadata": {},
   "source": [
    "**Task 4.** Calculate the derivative $f'(w)$ using PyTorch and backward propagation (the backward method of the Tensor class) for the following functions and points:\n",
    "  - $f(w) = w^3 + w^2$ and $w = 2.0$,\n",
    "  - $f(w) = \\text{sin}(w)$ and $w = \\pi$,\n",
    "  - $f(w) = \\ln(w * e^{3w})$ and $w = 1.0$.\n",
    "  \n",
    "Print the values of those derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "copyrighted-perry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "tensor(3.1416, requires_grad=True)\n",
      "tensor(1., requires_grad=True)\n",
      "out=12.0\n",
      "tensor(16.)\n",
      "out=-8.742277657347586e-08\n",
      "tensor(-1.)\n",
      "out=3.0\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "w_4_1 = torch.tensor(2.0, requires_grad=True)\n",
    "w_4_2 = torch.tensor(np.pi, requires_grad=True)\n",
    "w_4_3 = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "print(w_4_1)\n",
    "print(w_4_2)\n",
    "print(w_4_3)\n",
    "\n",
    "out_w_4_1 = w_4_1.pow(3) + w_4_1.pow(2)\n",
    "print(\"out={}\".format(out_w_4_1))\n",
    "out_w_4_1.backward()\n",
    "print(w_4_1.grad)\n",
    "\n",
    "out_w_4_2 = w_4_2.sin()\n",
    "print(\"out={}\".format(out_w_4_2))\n",
    "out_w_4_2.backward()\n",
    "print(w_4_2.grad)\n",
    "\n",
    "w_4_2.grad.data.zero_()\n",
    "\n",
    "out_w_4_3 = (((3 * w_4_3).exp()) * w_4_3).log()\n",
    "print(\"out={}\".format(out_w_4_3))\n",
    "out_w_4_3.backward()\n",
    "print(w_4_3.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-sarah",
   "metadata": {},
   "source": [
    "\n",
    "**Task 5.** Calculate the derivative $\\frac{\\partial f}{\\partial w_1}(w_1, w_2, w_3)$ using PyTorch and backward propagation (the backward method of the Tensor class) for the following functions and points:\n",
    "  - $f(w_1, w_2) = w_1^3 + w_1^2 + w_2$ and $(w_1, w_2) = (2.0, 3.0)$,\n",
    "  - $f(w_1, w_2, w_3) = \\text{sin}(w_1) * w_2 + w_1^2 * w_3$ and $(w_1, w_2) = (\\pi, 2.0, 4.0)$,\n",
    "  - $f(w_1, w_2, w_3) = e^{w_1^2 + w_2^2 + w_3^2} + w_1^2 + w_2^2 + w_3^2$ and $(w_1, w_2, w_3) = (0.5, 0.67, 0.55)$.\n",
    "  \n",
    "Print the values of those derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dietary-columbia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out=15.0\n",
      "tensor([16.,  1.])\n",
      "out=39.47842025756836\n",
      "tensor([ 2.3133e+01, -8.7423e-08,  9.8696e+00])\n",
      "out=3.723489999771118\n",
      "tensor([3.7221, 4.9876, 4.0943])\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "w_5_1 = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "w_5_2 = torch.tensor([np.pi, 2.0, 4.0], requires_grad=True)\n",
    "w_5_3 = torch.tensor([0.5, 0.67, 0.55], requires_grad=True)\n",
    "\n",
    "out_w_5_1 = (w_5_1[0]).pow(3) + (w_5_1[0]).pow(2) + w_5_1[1]\n",
    "print(\"out={}\".format(out_w_5_1))\n",
    "out_w_5_1.backward()\n",
    "print(w_5_1.grad)\n",
    "\n",
    "out_w_5_2 = (w_5_2[0]).sin() * w_5_2[1] + (w_5_2[0]).pow(2) * w_5_2[2]\n",
    "print(\"out={}\".format(out_w_5_2))\n",
    "out_w_5_2.backward()\n",
    "print(w_5_2.grad)\n",
    "\n",
    "out_w_5_3 = ((w_5_3[0]).pow(2) + (w_5_3[1]).pow(2) + (w_5_3[2]).pow(2)).exp() + (w_5_3[0]).pow(2) + (w_5_3[1]).pow(2) + (w_5_3[2]).pow(2)\n",
    "print(\"out={}\".format(out_w_5_3))\n",
    "out_w_5_3.backward()\n",
    "print(w_5_3.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-provider",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-margin",
   "metadata": {},
   "source": [
    "**Task 6.** Write the calculate_se_array(base_w, x, y_target, weight_idx, weight_start, weight_end) which generates a numpy array of squared errors for the following sigmoid function $\\text{sigmoid}(w * x)$ and target value $y_target$, where $x$ is a vector of dimension 3 and $w$ is a vector of weights of the same dimension. The function should generate a set of weight vectors w which have the same value as base_w on all coordinates other than weight_idx, and should have 100 values on the weight_idx coordinate ranging from weight_start to weight_end. Example for weight_idx =1: \n",
    "\n",
    "w = [[base_w[0], weight_start, base_w[2]], [base_w[0], weight_start + step, base_w[2]], ..., [base_w[0], weight_end - step, base_w[2]], [base_w[0], weight_end, base_w[2]]]\n",
    "\n",
    "Then the function should calculate and return an array of squared errors between the value of the sigmoid on x with respect to all those weight vectors and the target value y_target. This will allow you to see how the error of a single neuron depends on every weight separately.\n",
    "\n",
    "The function should work even if base_w and x are tensors (a safe way to do is tensor.cpu().detach().numpy()).\n",
    "\n",
    "You're encouraged to experiment with different neural nets to see how the error depends on all its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cloudy-growing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "def calculate_se_array(base_w, x, y_target, weight_idx, weight_start, weight_end):\n",
    "    ########################\n",
    "    # Write your code here #\n",
    "    ########################\n",
    "    sigmoid_result = torch.sigmoid(base_w * x)\n",
    "    step = (weight_end - weight_start) / 100\n",
    "    w = np.array([])\n",
    "    \n",
    "    temp = [0] * len(base_w)\n",
    "    \n",
    "    for i in range(len(base_w)):\n",
    "        temp[i] = base_w[i]\n",
    "    \n",
    "    for i in range(101):\n",
    "        print(type(w))\n",
    "        #if type(w) != np.ndarray:\n",
    "            \n",
    "         #   w = w.detach()\n",
    "        temp[weight_idx] = weight_start + i * step\n",
    "        w = w.cpu().detach().np().append(w, temp)\n",
    "        #w.append(temp)\n",
    "    return w\n",
    "\n",
    "test = np.array([])\n",
    "print(test)\n",
    "test = np.append(test, [1,2,3])\n",
    "print(test)\n",
    "\n",
    "'''\n",
    "poprawne\n",
    "def calculate_se_array(base_w, x, y_target, weight_idx, weight_start, weight_end):\n",
    "    if isinstance(base_w, torch.Tensor):\n",
    "        base_w = base_w.cpu().detach().numpy()\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.cpu().detach().numpy()\n",
    "        \n",
    "    w = np.linspace(weight_start, weight_end, 100)\n",
    "    if weight_idx == 0:\n",
    "        w_array = np.array([[w_single, base_w[1], base_w[2]] for w_single in w])\n",
    "    elif weight_idx == 1:\n",
    "        w_array = np.array([[base_w[0], w_single, base_w[2]] for w_single in w])\n",
    "    elif weight_idx == 2:\n",
    "        w_array = np.array([[base_w[0], base_w[1], w_single] for w_single in w])\n",
    "        \n",
    "    y = np.array([1 / (1 + np.exp(-np.sum(x * w))) for w in w_array])\n",
    "    \n",
    "    return np.power(y_target - y, 2)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-belarus",
   "metadata": {},
   "source": [
    "In the next cell you can test the method and in the cell after that you can see the error plots for the training you saw in the lecture. The red dot indicates the current weight value and the error it gives. The red dot should arrive at the lowest point at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "representative-sauce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-6a5de97a724d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mweight_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalculate_se_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_end\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-53-30ad216c0582>\u001b[0m in \u001b[0;36mcalculate_se_array\u001b[1;34m(base_w, x, y_target, weight_idx, weight_start, weight_end)\u001b[0m\n\u001b[0;32m     18\u001b[0m          \u001b[1;31m#   w = w.detach()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mweight_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight_start\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;31m#w.append(temp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2., -1., 3.], requires_grad=False)\n",
    "w = torch.tensor([-0.65, -0.4, -1.], requires_grad=True)\n",
    "y_target = 0.65\n",
    "\n",
    "base_w = w\n",
    "weight_idx = 0 \n",
    "weight_start = -5\n",
    "weight_end = 5\n",
    "\n",
    "print(calculate_se_array(base_w, x, y_target, weight_idx, weight_start, weight_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-sellers",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([2., -1., 3.], requires_grad=False)\n",
    "w = torch.tensor([-0.65, -0.4, -1.], requires_grad=True)\n",
    "y_target = 0.65\n",
    "\n",
    "optimizer = optim.SGD([w], lr=0.1)\n",
    "\n",
    "losses = []\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    y = torch.sigmoid(torch.sum(x * w))\n",
    "    loss = torch.pow(y - y_target, 2)\n",
    "    loss.backward()\n",
    "    losses.append(loss.item())\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch > 0 and epoch % 10 == 0:\n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        print(w.cpu().detach().numpy())\n",
    "        print(loss.item())\n",
    "        for i in range(3):    \n",
    "            w_range = np.linspace(-5, 5, 100)\n",
    "            errors = calculate_se_array(w, x, y_target, i, -5, 5)\n",
    "            sns.lineplot(x=w_range, y=errors).set_title('Training loss')\n",
    "            plt.scatter([w[i].item()], [loss.item()], color='red')\n",
    "            plt.xlabel(\"weight {}\".format(i))\n",
    "            plt.ylabel(\"squared loss\")\n",
    "            plt.show()\n",
    "            \n",
    "'''\n",
    "poprawne\n",
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self, seed):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        self.fc_1 = nn.Linear(2, 4, bias=False)\n",
    "        self.fc_2 = nn.Linear(4, 4, bias=False)\n",
    "        self.fc_3 = nn.Linear(4, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc_1(x))\n",
    "        x = torch.sigmoid(self.fc_2(x))\n",
    "        x = self.fc_3(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "\n",
    "# Initialize the data\n",
    "\n",
    "x_data = [torch.tensor([1.0, 1.0]), \n",
    "          torch.tensor([0.0, 0.0]), \n",
    "          torch.tensor([2.0, -1.0]), \n",
    "          torch.tensor([-1.0, 0.5]), \n",
    "          torch.tensor([-0.5, -2.0])\n",
    "         ]\n",
    "y_data = [torch.tensor(2.0),\n",
    "          torch.tensor(1.0),\n",
    "          torch.tensor(-1.0),\n",
    "          torch.tensor(0.5),\n",
    "          torch.tensor(2.0)]\n",
    "\n",
    "# Initialize the neural network and optimizer\n",
    "\n",
    "neural_net = TwoLayerModel(seed=6789)\n",
    "\n",
    "optimizer = optim.SGD(neural_net.parameters(), lr=0.1)\n",
    "\n",
    "# Perform the training for 1000 epochs\n",
    "\n",
    "losses = []\n",
    "n_epochs = 1000\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i in range(len(x_data)):\n",
    "        \n",
    "        y = neural_net(x_data[i])\n",
    "        if i == 0:\n",
    "            loss = torch.pow(y - y_data[i], 2)\n",
    "        else:\n",
    "            loss += torch.pow(y - y_data[i], 2)\n",
    "    \n",
    "    loss.backward()\n",
    "    losses.append(loss.item())\n",
    "        \n",
    "    optimizer.step()\n",
    "          \n",
    "print(\"All weights - layer by layer starting from left to right\")\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "for param in neural_net.parameters():\n",
    "    print(param.data)\n",
    "    \n",
    "print()\n",
    "print(\"w_{{1, 2}}^{{(1)}} (rounded to 4 decimal points)\")\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "print(np.round(list(neural_net.parameters())[0].data[0][1].item(), 4))\n",
    "\n",
    "print()\n",
    "print(\"w_{{1, 3}}^{{(2)}} (rounded to 4 decimal points)\")\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "print(np.round(list(neural_net.parameters())[2].data[0][2].item(), 4))\n",
    "\n",
    "print()\n",
    "print(\"Values on the training data (verify the difference is below epsilon)\")\n",
    "\n",
    "def f(x):\n",
    "    return x[0] * x[1] + 1\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "epsilon = 0.1\n",
    "for i in range(5):\n",
    "    assert np.abs(neural_net(x_data[i]).item() - f(x_data[i]).item()) < epsilon\n",
    "    print(neural_net(x_data[i]).item())\n",
    "    print(f(x_data[i]).item())\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print(\"NN value on x=[2.0, 2.0] vs real value\")\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "x = torch.tensor([2.0, 2.0])\n",
    "print(neural_net(x).item())\n",
    "print(f(x).item())\n",
    "\n",
    "print()\n",
    "print(\"NN value on x=[-1.0, -1.0] vs real value\")\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "x = torch.tensor([-1.0, -1.0])\n",
    "print(neural_net(x).item())\n",
    "print(f(x).item())\n",
    "\n",
    "print()\n",
    "print(\"NN value on x=[3.0, -3.0] vs real value\")\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "x = torch.tensor([3.0, -3.0])\n",
    "print(neural_net(x).item())\n",
    "print(f(x).item())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-border",
   "metadata": {},
   "source": [
    "**Task 7*.** Train a neural network with:\n",
    "  - two input neurons, \n",
    "  - four hidden neurons with no bias and sigmoid activation in the first hidden layer,\n",
    "  - four hidden neurons with no bias and with sigmoid activation in the second hidden layer,\n",
    "  - one output neuron with no bias and without sigmoid activation \n",
    "  \n",
    "to get a good approximation of $f(x) = x_1 * x_2 + 1$ on the following dataset $D = \\{(1.0, 1.0), (0.0, 0.0), (2.0, -1.0), (-1.0, 0.5), (-0.5, -2.0)\\}$, i.e. the network should satisfy:\n",
    "  - $\\text{net}(1.0, 1.0) \\sim 2.0$,\n",
    "  - $\\text{net}(0.0, 0.0) \\sim 1.0$,\n",
    "  - $\\text{net}(2.0, -1.0) \\sim -1.0$,\n",
    "  - $\\text{net}(-1.0, 0.5) \\sim 0.5$,\n",
    "  - $\\text{net}(-0.5, -2.0) \\sim 2.0$.\n",
    "  \n",
    "Use seed=6789 to initialize the network.\n",
    "\n",
    "After training print all weights and separately print $w_{1, 2}^{(1)}$ (the weight from the second input to the first hidden neuron in the first hidden layer) and $w_{1, 3}^{(3)}$ (the weight from the third hidden neuron in the second hidden layer to the output unit).\n",
    "\n",
    "Print the values of the network on the training points and verify that these values are closer to the real values of the $f$ function than $\\epsilon = 0.1$, i.e. $|\\text{net}(x) - f(x)| < \\epsilon$ for $x \\in D$.\n",
    "\n",
    "Because this network is only tested on the training set, it will certainly overfit if trained long enough. Train for 1000 epochs and then calculate\n",
    "  - $\\text{net}(2.0, 2.0)$,\n",
    "  - $\\text{net}(-1.0, -1.0)$,\n",
    "  - $\\text{net}(3.0, -3.0)$.\n",
    "  \n",
    "How far are these values from real values of the function $f$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self, seed):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        ########################\n",
    "        # Write your code here #\n",
    "        ########################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ########################\n",
    "        # Write your code here #\n",
    "        ########################\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "\n",
    "# Initialize the neural network and optimizer\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "\n",
    "# Perform the training for 1000 epochs\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "\n",
    "print(\"All weights - layer by layer starting from left to right\")\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "    \n",
    "print()\n",
    "print(\"w_{{1, 2}}^{{(1)}} (rounded to 4 decimal points)\")\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"w_{{1, 3}}^{{(2)}} (rounded to 4 decimal points)\")\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Values on the training data (verify the difference is below epsilon)\")\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"NN value on x=[2.0, 2.0] vs real value\")\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"NN value on x=[-1.0, -1.0] vs real value\")\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "print()\n",
    "print(\"NN value on x=[3.0, -3.0] vs real value\")\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
