{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "antique-certification",
   "metadata": {},
   "source": [
    "# Netflix recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display, HTML\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# Fix the dying kernel problem (only a problem in some installations - you can remove it, if it works without it)\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-tourist",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_ratings_df = pd.read_csv(os.path.join(\"data\", \"movielens_small\", \"ratings.csv\")).rename(columns={'userId': 'user_id', 'movieId': 'item_id'})\n",
    "ml_movies_df = pd.read_csv(os.path.join(\"data\", \"movielens_small\", \"movies.csv\")).rename(columns={'movieId': 'item_id'})\n",
    "ml_df = pd.merge(ml_ratings_df, ml_movies_df, on='item_id')\n",
    "\n",
    "# Filter the data to reduce the number of movies\n",
    "seed = 6789\n",
    "rng = np.random.RandomState(seed=seed)\n",
    "left_ids = rng.choice(ml_movies_df['item_id'], size=90, replace=False)\n",
    "left_ids = list(set(left_ids).union(set([1, 318, 1193, 1208, 1214, 1721, 2959, 3578, 4306, 109487])))\n",
    "\n",
    "ml_ratings_df = ml_ratings_df.loc[ml_ratings_df['item_id'].isin(left_ids)]\n",
    "ml_movies_df = ml_movies_df.loc[ml_movies_df['item_id'].isin(left_ids)]\n",
    "ml_df = ml_df.loc[ml_df['item_id'].isin(left_ids)]\n",
    "\n",
    "display(ml_movies_df.head(10))\n",
    "\n",
    "print(\"Number of interactions left: {}\".format(len(ml_ratings_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-brooklyn",
   "metadata": {},
   "source": [
    "## Shift item ids and user ids so that they are consecutive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = ml_ratings_df.copy()\n",
    "\n",
    "unique_item_ids = interactions_df['item_id'].unique()\n",
    "item_id_mapping = dict(zip(unique_item_ids, list(range(len(unique_item_ids)))))\n",
    "item_id_reverse_mapping = dict(zip(list(range(len(unique_item_ids))), unique_item_ids))\n",
    "unique_user_ids = interactions_df['user_id'].unique()\n",
    "user_id_mapping = dict(zip(unique_user_ids, list(range(len(unique_user_ids)))))\n",
    "user_id_reverse_mapping = dict(zip(list(range(len(unique_user_ids))), unique_user_ids))\n",
    "\n",
    "interactions_df['item_id'] = interactions_df['item_id'].map(item_id_mapping)\n",
    "interactions_df['user_id'] = interactions_df['user_id'].map(user_id_mapping)\n",
    "\n",
    "display(interactions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-meeting",
   "metadata": {},
   "source": [
    "## Get the number of items and users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = np.max(interactions_df['item_id']) + 1\n",
    "n_users = np.max(interactions_df['user_id']) + 1\n",
    "\n",
    "print(\"n_items={}\\nn_users={}\".format(n_items, n_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-threshold",
   "metadata": {},
   "source": [
    "## Get the user-item interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping to int is necessary because of how iterrows works\n",
    "r = np.zeros(shape=(n_users, n_items))\n",
    "for idx, interaction in interactions_df.iterrows():\n",
    "    r[int(interaction['user_id'])][int(interaction['item_id'])] = 1\n",
    "    \n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-little",
   "metadata": {},
   "source": [
    "## Generate negative interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-crime",
   "metadata": {},
   "source": [
    "**Task 1.** Generate negative interactions, i.e. such pairs of user_id, item_id which do not appear in the original interactions (interactions_df). For every positive interaction generate n_neg_per_pos negative interactions. Store those pairs as tuples (user_id, item_id, 0) in the negative_interactions list. The last position in the tuple is an indicator if this was a positive or a negative interaction. Finally transform the negative_interactions into a DataFrame with columns 'user_id', 'item_id', 'interacted' and concatenate it to the end of the interactions_pos_neg_df DataFrame.\n",
    "\n",
    "Try to find the most efficient way to generate those negative interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neg_per_pos = 5\n",
    "interactions_pos_neg_df = interactions_df[['user_id', 'item_id']].copy()\n",
    "\n",
    "# Indicate positive interactions\n",
    "\n",
    "interactions_pos_neg_df['interacted'] = 1\n",
    "\n",
    "# Generate negative interactions\n",
    "\n",
    "negative_interactions = []\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-virgin",
   "metadata": {},
   "source": [
    "**Task 2.** Initialize user and item embeddings of size embedding_dim. User embeddings should be stored as a numpy array user_repr_matrix of size (n_users, embedding_dim), while item embeddings in an array item_repr_matrix of size (n_items, embedding_dim). Both matrices should initialized from the Gaussian distribution with mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-profession",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_dim = 2\n",
    "rng = np.random.RandomState(seed=seed)\n",
    "\n",
    "########################\n",
    "# Write your code here #\n",
    "########################\n",
    "\n",
    "print(user_repr_matrix)\n",
    "print()\n",
    "print(item_repr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-soundtrack",
   "metadata": {},
   "source": [
    "**Task 3.** Write the perform_mf_sgd_step method which takes user representation user_repr for a single user (a 1D numpy array), item representation item_repr for a single item (a 1D numpy array), interaction value r_ui which is a binary value to be predicted (the value from the interaction matrix), learning rate lr, regularization constant reg_l, and performs a single step of the stochastic gradient descent for matrix factorization as described in the Koren, Bell, Volinksy \"Matrix Factorization Techniques for Recommender Systems\". The method should return a tuple (user_repr, item_repr, loss) of new representations and the quadratic loss which was minimized (loss before the update):\n",
    "\n",
    "<center>\n",
    "$$\n",
    "    loss = (r_{ui} - user\\_repr * item\\_repr)^2\n",
    "$$\n",
    "</center>\n",
    "where the mutliplication sign denotes the scalar product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_mf_sgd_step(user_repr, item_repr, r_ui, lr, reg_l):\n",
    "    ########################\n",
    "    # Write your code here #\n",
    "    ########################\n",
    "\n",
    "\n",
    "# Test\n",
    "\n",
    "user_repr = np.array([0.25, -0.33])\n",
    "item_repr = np.array([0.75, 0.15])\n",
    "r_ui = 1\n",
    "lr = 0.01\n",
    "reg_l = 0.1\n",
    "user_repr, item_repr, loss = perform_mf_sgd_step(user_repr, item_repr, r_ui, lr, reg_l)\n",
    "print(user_repr, item_repr, loss)\n",
    "assert np.abs(user_repr[0] - 0.256215) < 0.001\n",
    "assert np.abs(user_repr[1] - -0.328377) < 0.001\n",
    "assert np.abs(item_repr[0] - 0.75145857) < 0.001\n",
    "assert np.abs(item_repr[1] - 0.14701939) < 0.001\n",
    "assert np.abs(loss - 0.7430439999999999) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-sword",
   "metadata": {},
   "source": [
    "**Task 4.** Write the perform_mf_sgd_epoch method which takes interactions_df, user_repr_matrix, item_repr_matrix, learning rate lr, regularization constant reg_l as input, iterates over all rows of interactions_df, performs perform_mf_sgd_step for every row (remember that every row of interactions_df contains user_id, item_id (already consecutive hence they can be used as indices of the user representations and item representations matrices) and the interacted column which is the value to be predicted - r_ui), updates the appropriate user and item representations in the user_repr matrix, item_repr matrix matrices and increases total_loss by the loss returned by the perform_mf_sgd_step method, and finally returns a tuple (user_repr_matrix, item_repr_matrix, total_loss).\n",
    "\n",
    "To obtain consistent results and pass the assertion run the cell of task 2 again before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-upset",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def perform_mf_sgd_epoch(interactions_df, user_repr_matrix, item_repr_matrix, lr, reg_l):\n",
    "    ########################\n",
    "    # Write your code here #\n",
    "    ########################\n",
    "    \n",
    "\n",
    "# Test\n",
    "\n",
    "total_loss = 0\n",
    "user_repr_matrix, item_repr_matrix, total_loss \\\n",
    "    = perform_mf_sgd_epoch(interactions_pos_neg_df, user_repr_matrix, item_repr_matrix, lr, reg_l)\n",
    "\n",
    "print(user_repr_matrix)\n",
    "print()\n",
    "print(item_repr_matrix)\n",
    "print()\n",
    "print(total_loss)\n",
    "\n",
    "assert np.abs(total_loss - 5076.2638779478975) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-velvet",
   "metadata": {},
   "source": [
    "**Task 5.** Write the perform_mf_sgd_training method which takes interactions_df, user_repr_matrix, item_repr_matrix, n_epochs, learning rate lr, regularization constant reg_l as input, permutates the rows of interactions_df (with rng.permutation) and performs perform_mf_sgd_epoch n_epochs times, and finally returns trained user and item representations and the final loss obtained in the last epoch (user_repr_matrix, item_repr_matrix, training_last_avg_loss).\n",
    "\n",
    "To obtain consistent results and pass the assertion run the cell of tasks 2, 4 again before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-recording",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "\n",
    "def perform_mf_sgd_training(interactions_df, user_repr_matrix, item_repr_matrix, n_epochs, lr, reg_l):\n",
    "    liveloss = PlotLosses()\n",
    "    \n",
    "    ########################\n",
    "    # Write your code here #\n",
    "    ########################\n",
    "    \n",
    "        # Save and print epoch losses (this should be at the end of the loop over epochs)\n",
    "\n",
    "        training_last_avg_loss = total_loss / len(interactions_df)\n",
    "\n",
    "        if epoch >= 3: # A bound on epoch prevents showing extremely high losses in the first epochs\n",
    "            logs = {'loss': training_last_avg_loss}\n",
    "            liveloss.update(logs)\n",
    "            liveloss.send()\n",
    "\n",
    "            \n",
    "# Test\n",
    "\n",
    "n_epochs = 100\n",
    "user_repr_matrix, item_repr_matrix, training_last_avg_loss \\\n",
    "    = perform_mf_sgd_training(interactions_pos_neg_df, user_repr_matrix, item_repr_matrix, n_epochs, lr, reg_l)\n",
    "\n",
    "print(user_repr_matrix)\n",
    "print()\n",
    "print(item_repr_matrix)\n",
    "print()\n",
    "print(training_last_avg_loss)\n",
    "\n",
    "# assert np.abs(total_loss - 5076.2638779478975) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-tennis",
   "metadata": {},
   "source": [
    "### Plot movie representations\n",
    "\n",
    "Remember that they don't have to be good as only two dimensions have been used. But still try to find if you can assign any meaning to those dimensions based on your knowledge about plotted movies. You can open the image in new tab and enlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(tight_layout=True)\n",
    "fig.set_size_inches(64, 36)\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "sns.scatterplot(x=item_repr_matrix[:, 0], y=item_repr_matrix[:, 1], ax=ax1)\n",
    "\n",
    "for i in range(len(item_repr_matrix)):\n",
    "    title = ml_movies_df.loc[ml_movies_df['item_id'] == item_id_reverse_mapping[i], 'title'].iloc[0]\n",
    "    plt.text(x=item_repr_matrix[i, 0] + 1 / 150, y=item_repr_matrix[i, 1] + 1 / 150, s=title, \n",
    "             fontdict=dict(color='red', size=8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-powell",
   "metadata": {},
   "source": [
    "**Task 6.** Write the recommend method which takes user_id, interactions_df, user_repr_matrix, item_repr_matrix as input and returns the best recommendation (item_id) and its score for the given user based on scores calculated as dot products of the user representation (from user_repr_matrix) and item representations (item_repr_matrix). Remember to map the user id with user_id_mapping. Do not include films the user has already watched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_id, interactions_df, user_repr_matrix, item_repr_matrix):\n",
    "    ########################\n",
    "    # Write your code here #\n",
    "    ########################\n",
    "\n",
    "\n",
    "# Test\n",
    "\n",
    "user_id = 1\n",
    "item_id, score = recommend(user_id, interactions_df, user_repr_matrix, item_repr_matrix)\n",
    "item_id = item_id_reverse_mapping[item_id]\n",
    "title = ml_movies_df.loc[ml_movies_df['item_id'] == item_id, 'title'].iloc[0]\n",
    "\n",
    "print(\"Best item id: {}\".format(item_id))\n",
    "print(\"Best item title: {}\".format(title))\n",
    "print(\"Best item score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-prediction",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent Netflix Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "\n",
    "from recommenders.recommender import Recommender\n",
    "\n",
    "\n",
    "class NetflixRecommender(Recommender):\n",
    "    \"\"\"\n",
    "    Collaborative filtering based on matrix factorization with the following choice of an optimizer:\n",
    "      - Stochastic Gradient Descent (SGD),\n",
    "      - Mini-Batch Gradient Descent (MBGD),\n",
    "      - Alternating Least Squares (ALS).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seed=6789, n_neg_per_pos=5, print_type=None, **params):\n",
    "        super().__init__()\n",
    "        self.recommender_df = pd.DataFrame(columns=['user_id', 'item_id', 'score'])\n",
    "        self.interactions_df = None\n",
    "        self.item_id_mapping = None\n",
    "        self.user_id_mapping = None\n",
    "        self.item_id_reverse_mapping = None\n",
    "        self.user_id_reverse_mapping = None\n",
    "        self.r = None\n",
    "        self.most_popular_items = None\n",
    "        \n",
    "        self.n_neg_per_pos = n_neg_per_pos\n",
    "        if 'optimizer' in params:\n",
    "            self.optimizer = params['optimizer']\n",
    "        else:\n",
    "            self.optimizer = 'SGD'\n",
    "        if 'n_epochs' in params:  # number of epochs (each epoch goes through the entire training set)\n",
    "            self.n_epochs = params['n_epochs']\n",
    "        else:\n",
    "            self.n_epochs = 10\n",
    "        if 'lr' in params:  # learning rate\n",
    "            self.lr = params['lr']\n",
    "        else:\n",
    "            self.lr = 0.01\n",
    "        if 'reg_l' in params:  # regularization coefficient\n",
    "            self.reg_l = params['reg_l']\n",
    "        else:\n",
    "            self.reg_l = 0.1\n",
    "        if 'embedding_dim' in params:\n",
    "            self.embedding_dim = params['embedding_dim']\n",
    "        else:\n",
    "            self.embedding_dim = 8\n",
    "        \n",
    "        self.user_repr = None\n",
    "        self.item_repr = None\n",
    "\n",
    "        if 'should_recommend_already_bought' in params:\n",
    "            self.should_recommend_already_bought = params['should_recommend_already_bought']\n",
    "        else:\n",
    "            self.should_recommend_already_bought = False\n",
    "            \n",
    "        self.validation_set_size = 0.2\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.rng = np.random.RandomState(seed=seed)  \n",
    "        \n",
    "        self.print_type = print_type\n",
    "\n",
    "    def fit(self, interactions_df, users_df, items_df):\n",
    "        \"\"\"\n",
    "        Training of the recommender.\n",
    "\n",
    "        :param pd.DataFrame interactions_df: DataFrame with recorded interactions between users and items\n",
    "            defined by user_id, item_id and features of the interaction.\n",
    "        :param pd.DataFrame users_df: DataFrame with users and their features defined by\n",
    "            user_id and the user feature columns.\n",
    "        :param pd.DataFrame items_df: DataFrame with items and their features defined\n",
    "            by item_id and the item feature columns.\n",
    "        \"\"\"\n",
    "\n",
    "        del users_df, items_df\n",
    "\n",
    "        # Shift item ids and user ids so that they are consecutive\n",
    "\n",
    "        unique_item_ids = interactions_df['item_id'].unique()\n",
    "        self.item_id_mapping = dict(zip(unique_item_ids, list(range(len(unique_item_ids)))))\n",
    "        self.item_id_reverse_mapping = dict(zip(list(range(len(unique_item_ids))), unique_item_ids))\n",
    "        unique_user_ids = interactions_df['user_id'].unique()\n",
    "        self.user_id_mapping = dict(zip(unique_user_ids, list(range(len(unique_user_ids)))))\n",
    "        self.user_id_reverse_mapping = dict(zip(list(range(len(unique_user_ids))), unique_user_ids))\n",
    "\n",
    "        interactions_df = interactions_df.copy()\n",
    "        interactions_df['item_id'] = interactions_df['item_id'].map(self.item_id_mapping)\n",
    "        interactions_df['user_id'] = interactions_df['user_id'].map(self.user_id_mapping)\n",
    "\n",
    "        # Get the number of items and users\n",
    "\n",
    "        self.interactions_df = interactions_df\n",
    "        n_users = np.max(interactions_df['user_id']) + 1\n",
    "        n_items = np.max(interactions_df['item_id']) + 1\n",
    "\n",
    "        # Get the user-item interaction matrix (mapping to int is necessary because of how iterrows works)\n",
    "        r = np.zeros(shape=(n_users, n_items))\n",
    "        for idx, interaction in interactions_df.iterrows():\n",
    "            r[int(interaction['user_id'])][int(interaction['item_id'])] = 1\n",
    "\n",
    "        self.r = r\n",
    "        \n",
    "        # Indicate positive interactions\n",
    "        \n",
    "        interactions_df['interacted'] = 1\n",
    "\n",
    "        # Generate negative interactions\n",
    "        negative_interactions = []\n",
    "\n",
    "        i = 0\n",
    "        while i < self.n_neg_per_pos * len(interactions_df):\n",
    "            sample_size = 1000\n",
    "            user_ids = self.rng.choice(np.arange(n_users), size=sample_size)\n",
    "            item_ids = self.rng.choice(np.arange(n_items), size=sample_size)\n",
    "\n",
    "            j = 0\n",
    "            while j < sample_size and i < self.n_neg_per_pos * len(interactions_df):\n",
    "                if r[user_ids[j]][item_ids[j]] == 0:\n",
    "                    negative_interactions.append([user_ids[j], item_ids[j], 0])\n",
    "                    i += 1\n",
    "                j += 1\n",
    "        \n",
    "        interactions_df = pd.concat(\n",
    "            [interactions_df, pd.DataFrame(negative_interactions, columns=['user_id', 'item_id', 'interacted'])])\n",
    "        \n",
    "        # Initialize user and item embeddings as random vectors (from Gaussian distribution)\n",
    "        \n",
    "        self.user_repr = self.rng.normal(0, 1, size=(r.shape[0], self.embedding_dim))\n",
    "        self.item_repr = self.rng.normal(0, 1, size=(r.shape[1], self.embedding_dim))\n",
    "        \n",
    "        # Initialize losses and loss visualization\n",
    "        \n",
    "        if self.print_type is not None and self.print_type == 'live':\n",
    "            liveloss = PlotLosses()\n",
    "\n",
    "        training_losses = deque(maxlen=50)\n",
    "        training_avg_losses = []\n",
    "        training_epoch_losses = []\n",
    "        validation_losses = deque(maxlen=50)\n",
    "        validation_avg_losses = []\n",
    "        validation_epoch_losses = []\n",
    "        last_training_total_loss = 0.0\n",
    "        last_validation_total_loss = 0.0\n",
    "        \n",
    "        # Split the data\n",
    "        \n",
    "        interaction_ids = self.rng.permutation(len(interactions_df))\n",
    "        train_validation_slice_idx = int(len(interactions_df) * (1 - self.validation_set_size))\n",
    "        training_ids = interaction_ids[:train_validation_slice_idx]\n",
    "        validation_ids = interaction_ids[train_validation_slice_idx:]\n",
    "        \n",
    "        # Train the model\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            if self.print_type is not None and self.print_type == 'live':\n",
    "                logs = {}\n",
    "            \n",
    "            # Train\n",
    "            \n",
    "            training_losses.clear()\n",
    "            training_total_loss = 0.0\n",
    "            batch_idx = 0\n",
    "            for idx in training_ids:\n",
    "                user_id = int(interactions_df.iloc[idx]['user_id'])\n",
    "                item_id = int(interactions_df.iloc[idx]['item_id'])\n",
    "                r_ui = interactions_df.iloc[idx]['interacted']\n",
    "            \n",
    "                e_ui = r_ui - np.dot(self.user_repr[user_id], self.item_repr[item_id])\n",
    "                self.user_repr[user_id] = self.user_repr[user_id] \\\n",
    "                    + self.lr * (e_ui * self.item_repr[item_id] - self.reg_l * self.user_repr[user_id])\n",
    "                self.item_repr[item_id] = self.item_repr[item_id] \\\n",
    "                    + self.lr * (e_ui * self.user_repr[user_id] - self.reg_l * self.item_repr[item_id])\n",
    "                \n",
    "                loss = e_ui**2\n",
    "                training_total_loss += loss\n",
    "                \n",
    "                if self.print_type is not None and self.print_type == 'text':\n",
    "                    print('\\rEpoch: {}\\tBatch: {}\\tLast epoch - avg training loss: {:.2f} avg validation loss: {:.2f} loss: {}'.format(\n",
    "                        epoch, batch_idx, last_training_total_loss, last_validation_total_loss, loss), end=\"\")\n",
    "                    \n",
    "                batch_idx += 1\n",
    "                \n",
    "                training_losses.append(loss)\n",
    "                training_avg_losses.append(np.mean(training_losses))\n",
    "                \n",
    "            # Validate\n",
    "            \n",
    "            validation_losses.clear()\n",
    "            validation_total_loss = 0.0\n",
    "            for idx in validation_ids:\n",
    "                user_id = int(interactions_df.iloc[idx]['user_id'])\n",
    "                item_id = int(interactions_df.iloc[idx]['item_id'])\n",
    "            \n",
    "                e_ui = r[user_id, item_id] - np.dot(self.user_repr[user_id], self.item_repr[item_id])\n",
    "                \n",
    "                loss = e_ui**2\n",
    "                validation_total_loss += loss\n",
    "\n",
    "                validation_losses.append(loss)\n",
    "                validation_avg_losses.append(np.mean(validation_losses))\n",
    "                \n",
    "            # Save and print epoch losses\n",
    "            \n",
    "            training_last_avg_loss = training_total_loss / len(training_ids)\n",
    "            validation_last_avg_loss = validation_total_loss / len(validation_ids)\n",
    "\n",
    "            if self.print_type is not None and self.print_type == 'live' and epoch >= 3:\n",
    "                # A bound on epoch prevents showing extremely high losses in the first epochs\n",
    "                logs['loss'] = training_last_avg_loss\n",
    "                logs['val_loss'] = validation_last_avg_loss\n",
    "                liveloss.update(logs)\n",
    "                liveloss.send()\n",
    "\n",
    "        # Find the most popular items for the cold start problem\n",
    "\n",
    "        offers_count = interactions_df.loc[:, ['item_id', 'user_id']].groupby(by='item_id').count()\n",
    "        offers_count = offers_count.sort_values('user_id', ascending=False)\n",
    "        self.most_popular_items = offers_count.index\n",
    "\n",
    "    def recommend(self, users_df, items_df, n_recommendations=1):\n",
    "        \"\"\"\n",
    "        Serving of recommendations. Scores items in items_df for each user in users_df and returns\n",
    "        top n_recommendations for each user.\n",
    "\n",
    "        :param pd.DataFrame users_df: DataFrame with users and their features for which\n",
    "            recommendations should be generated.\n",
    "        :param pd.DataFrame items_df: DataFrame with items and their features which should be scored.\n",
    "        :param int n_recommendations: Number of recommendations to be returned for each user.\n",
    "        :return: DataFrame with user_id, item_id and score as columns returning n_recommendations top recommendations\n",
    "            for each user.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        # Clean previous recommendations (iloc could be used alternatively)\n",
    "        self.recommender_df = self.recommender_df[:0]\n",
    "\n",
    "        # Handle users not in the training data\n",
    "\n",
    "        # Map item ids\n",
    "\n",
    "        items_df = items_df.copy()\n",
    "        items_df = items_df.loc[items_df['item_id'].isin(self.item_id_mapping)]\n",
    "        items_df.replace({'item_id': self.item_id_mapping}, inplace=True)\n",
    "\n",
    "        # Generate recommendations\n",
    "\n",
    "        for idx, user in users_df.iterrows():\n",
    "            recommendations = []\n",
    "\n",
    "            user_id = user['user_id']\n",
    "\n",
    "            if user_id in self.user_id_mapping:\n",
    "                mapped_user_id = self.user_id_mapping[user_id]\n",
    "                \n",
    "                ids_list = items_df['item_id'].tolist()\n",
    "                id_to_pos = np.array([0]*len(ids_list))\n",
    "                for k in range(len(ids_list)):\n",
    "                    id_to_pos[ids_list[k]] = k\n",
    "                scores = np.matmul(self.user_repr[mapped_user_id].reshape(1, -1), \n",
    "                                   self.item_repr[ids_list].T).flatten()\n",
    "                \n",
    "                # Choose n recommendations based on highest scores\n",
    "                if not self.should_recommend_already_bought:\n",
    "                    x_list = self.interactions_df.loc[\n",
    "                        self.interactions_df['user_id'] == mapped_user_id]['item_id'].tolist()\n",
    "                    scores[id_to_pos[x_list]] = -1e100\n",
    "\n",
    "                chosen_pos = np.argsort(-scores)[:n_recommendations]\n",
    "\n",
    "                for item_pos in chosen_pos:\n",
    "                    recommendations.append(\n",
    "                        {\n",
    "                            'user_id': self.user_id_reverse_mapping[mapped_user_id],\n",
    "                            'item_id': self.item_id_reverse_mapping[ids_list[item_pos]],\n",
    "                            'score': scores[item_pos]\n",
    "                        }\n",
    "                    )\n",
    "            else:  # For new users recommend most popular items\n",
    "                for i in range(n_recommendations):\n",
    "                    recommendations.append(\n",
    "                        {\n",
    "                            'user_id': user['user_id'],\n",
    "                            'item_id': self.item_id_reverse_mapping[self.most_popular_items[i]],\n",
    "                            'score': 1.0\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            user_recommendations = pd.DataFrame(recommendations)\n",
    "\n",
    "            self.recommender_df = pd.concat([self.recommender_df, user_recommendations])\n",
    "\n",
    "        return self.recommender_df\n",
    "    \n",
    "    def get_user_repr(self, user_id):\n",
    "        mapped_user_id = self.user_id_mapping[user_id]\n",
    "        return self.user_repr[mapped_user_id]\n",
    "    \n",
    "    def get_item_repr(self, item_id):\n",
    "        mapped_item_id = self.item_id_mapping[item_id]\n",
    "        return self.item_repr[mapped_item_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-offering",
   "metadata": {},
   "source": [
    "## Quick test of the recommender (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_recommender = NetflixRecommender(print_type='live', embedding_dim=8, n_epochs=200)\n",
    "netflix_recommender.fit(ml_ratings_df, None, ml_movies_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-messaging",
   "metadata": {},
   "source": [
    "## Quick test of the recommender (recommending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = netflix_recommender.recommend(pd.DataFrame([[1], [4], [6]], columns=['user_id']), ml_movies_df, 10)\n",
    "\n",
    "recommendations = pd.merge(recommendations, ml_movies_df, on='item_id', how='left')\n",
    "print(\"Recommendations\")\n",
    "display(HTML(recommendations.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-barcelona",
   "metadata": {},
   "source": [
    "## User and item representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 1\n",
    "user_repr = netflix_recommender.get_user_repr(user_id=user_id)\n",
    "print(\"User id={}\".format(user_id))\n",
    "print(user_repr)\n",
    "print()\n",
    "\n",
    "print(\"User watched\")\n",
    "print(ml_df.loc[ml_df['user_id'] == user_id, 'title'].tolist())\n",
    "print()\n",
    "\n",
    "print('User history item representations')\n",
    "for item_id in ml_df.loc[ml_df['user_id'] == user_id, 'item_id'].tolist():\n",
    "    item_repr = netflix_recommender.get_item_repr(item_id=item_id)\n",
    "    print(\"Item id = {}\\titem title = {}\".format(item_id, ml_movies_df.loc[ml_movies_df['item_id'] == item_id, 'title'].iloc[0]))\n",
    "    print(item_repr)\n",
    "    score = np.dot(user_repr, item_repr)\n",
    "    print(\"Score={:.6f}\".format(score))\n",
    "    print()\n",
    "\n",
    "print(\"===============\")\n",
    "    \n",
    "item_id = 145\n",
    "item_repr = netflix_recommender.get_item_repr(item_id=item_id)\n",
    "print(\"Item id = {}\\titem title = {}\".format(item_id, ml_movies_df.loc[ml_movies_df['item_id'] == item_id, 'title'].iloc[0]))\n",
    "print(item_repr)\n",
    "score = np.dot(user_repr, item_repr)\n",
    "print(\"Score={:.6f}\".format(score))\n",
    "print()\n",
    "\n",
    "item_id = 171\n",
    "item_repr = netflix_recommender.get_item_repr(item_id=item_id)\n",
    "print(\"Item id = {}\\titem title = {}\".format(item_id, ml_movies_df.loc[ml_movies_df['item_id'] == item_id, 'title'].iloc[0]))\n",
    "print(item_repr)\n",
    "score = np.dot(user_repr, item_repr)\n",
    "print(\"Score={:.6f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-negative",
   "metadata": {},
   "source": [
    "# Training-test split evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_and_testing.testing import evaluate_train_test_split_implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_recommender = NetflixRecommender(n_epochs=150)\n",
    "\n",
    "netflix_tts_results = [['NetflixRecommender'] + list(evaluate_train_test_split_implicit(\n",
    "    netflix_recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df))]\n",
    "\n",
    "netflix_tts_results = pd.DataFrame(\n",
    "    netflix_tts_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(netflix_tts_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.amazon_recommender import AmazonRecommender\n",
    "\n",
    "amazon_recommender = AmazonRecommender()\n",
    "\n",
    "amazon_tts_results = [['AmazonRecommender'] + list(evaluate_train_test_split_implicit(\n",
    "    amazon_recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df))]\n",
    "\n",
    "amazon_tts_results = pd.DataFrame(\n",
    "    amazon_tts_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(amazon_tts_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.tfidf_recommender import TFIDFRecommender\n",
    "\n",
    "tfidf_recommender = TFIDFRecommender()\n",
    "\n",
    "tfidf_tts_results = [['TFIDFRecommender'] + list(evaluate_train_test_split_implicit(\n",
    "    tfidf_recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df))]\n",
    "\n",
    "tfidf_tts_results = pd.DataFrame(\n",
    "    tfidf_tts_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(tfidf_tts_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_results = pd.concat([netflix_tts_results, amazon_tts_results, tfidf_tts_results]).reset_index(drop=True)\n",
    "display(HTML(tts_results.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-harassment",
   "metadata": {},
   "source": [
    "# Leave-one-out evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_and_testing.testing import evaluate_leave_one_out_implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_recommender = NetflixRecommender(n_epochs=10)\n",
    "\n",
    "netflix_loo_results = [['NetflixRecommender'] + list(evaluate_leave_one_out_implicit(\n",
    "    netflix_recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df, max_evals=300, seed=6789))]\n",
    "\n",
    "netflix_loo_results = pd.DataFrame(\n",
    "    netflix_loo_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(netflix_loo_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.amazon_recommender import AmazonRecommender\n",
    "\n",
    "amazon_recommender = AmazonRecommender()\n",
    "\n",
    "amazon_loo_results = [['AmazonRecommender'] + list(evaluate_leave_one_out_implicit(\n",
    "    amazon_recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df, max_evals=300, seed=6789))]\n",
    "\n",
    "amazon_loo_results = pd.DataFrame(\n",
    "    amazon_loo_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(amazon_loo_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_recommender = TFIDFRecommender()\n",
    "\n",
    "tfidf_loo_results = [['TFIDFRecommender'] + list(evaluate_leave_one_out_implicit(\n",
    "    tfidf_recommender, ml_ratings_df.loc[:, ['user_id', 'item_id']], ml_movies_df, max_evals=300, seed=6789))]\n",
    "\n",
    "tfidf_loo_results = pd.DataFrame(\n",
    "    tfidf_loo_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(HTML(tfidf_loo_results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "loo_results = pd.concat([netflix_loo_results, amazon_loo_results, tfidf_loo_results]).reset_index(drop=True)\n",
    "display(HTML(loo_results.to_html()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
